{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > 12.326\n",
      " > 12.627\n",
      " > 9.171\n",
      " > 12.943\n",
      " > 10.312\n",
      " > 11.493\n",
      " > 10.425\n",
      " > 11.136\n",
      " > 10.824\n",
      " > 11.642\n",
      " > 10.924\n",
      " > 10.651\n",
      " > 9.577\n",
      " > 10.917\n",
      " > 9.789\n",
      " > 10.476\n",
      " > 10.660\n",
      " > 11.613\n",
      " > 12.114\n",
      " > 11.047\n",
      " > 10.731\n",
      " > 12.174\n",
      " > 11.330\n",
      " > 11.000\n",
      " > 10.535\n",
      " > 11.678\n",
      " > 10.896\n",
      " > 11.474\n",
      " > 11.220\n",
      " > 13.479\n",
      "mlp: 11.173 RMSE (+/- 0.939)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > 9.892\n",
      "[175.72705]\n"
     ]
    }
   ],
   "source": [
    "'''# load and evaluate a saved model\n",
    "import tensorflow as tf\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "\n",
    "model = tf.keras.models.load_model('model.h5')\n",
    "# summarize model.\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.summary()\n",
    "v = np.array([2020-12-23,2019-12-23,2019-12-23,2019-12-23,2019-12-23,2019-12-23,2019-12-23,2019-12-23,2019-12-23,2019-12-23,2019-12-23,2019-12-23,2019-12-23,2019-12-23,2019-12-23,2019-12-23,2019-12-23,2019-12-23,2019-12-23,2019-12-23,2019-12-23,2019-12-23,2019-12-23,2019-12-23])\n",
    "x_input = array(v).reshape(1, 24)\n",
    "result = model.predict(x_input, verbose=0)\n",
    "print (result[0])'''\n",
    "from math import sqrt\n",
    "from numpy import array\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# split a univariate dataset into train/test sets\n",
    "def train_test_split(data, n_test):\n",
    "  return data[:-n_test], data[-n_test:]\n",
    "\n",
    "def series_to_supervised(data, n_in, n_out=1):\n",
    "  df = DataFrame(data)\n",
    "  cols = list()\n",
    "  # input sequence (t-n, ... t-1)\n",
    "  for i in range(n_in, 0, -1):\n",
    "    cols.append(df.shift(i))\n",
    "  # forecast sequence (t, t+1, ... t+n)\n",
    "  for i in range(0, n_out):\n",
    "    cols.append(df.shift(-i))\n",
    "  # put it all together\n",
    "  agg = concat(cols, axis=1)\n",
    "  # drop rows with NaN values\n",
    "  agg.dropna(inplace=True)\n",
    "  return agg.values\n",
    "\n",
    "# root mean squared error or rmse\n",
    "def measure_rmse(actual, predicted):\n",
    "  return sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "# fit a model\n",
    "def model_fit(train, config):\n",
    "  # unpack config\n",
    "  n_input, n_nodes, n_epochs, n_batch = config\n",
    "  # prepare data\n",
    "  data = series_to_supervised(train, n_input)\n",
    "  train_x, train_y = data[:, :-1], data[:, -1]\n",
    "  # define model\n",
    "  model = Sequential()\n",
    "  model.add(Dense(n_nodes, activation='relu', input_dim=n_input))\n",
    "  model.add(Dense(1))\n",
    "  model.compile(loss='mse', optimizer='adam')\n",
    "  # fit\n",
    "  model.fit(train_x, train_y, epochs=n_epochs, batch_size=n_batch, verbose=0)\n",
    "  return model\n",
    "\n",
    "# forecast with a pre-fit model\n",
    "def model_predict(model, history, config):\n",
    "  # unpack config\n",
    "  n_input, _, _, _ = config\n",
    "  # prepare data\n",
    "  x_input = array(history[-n_input:]).reshape(1, n_input)\n",
    "  # forecast\n",
    "  yhat = model.predict(x_input, verbose=0)\n",
    "  return yhat[0]\n",
    "\n",
    "# walk-forward validation for univariate data\n",
    "def walk_forward_validation(data, n_test, cfg):\n",
    "  predictions = list()\n",
    "  # split dataset\n",
    "  train, test = train_test_split(data, n_test)\n",
    "  # fit model\n",
    "  model = model_fit(train, cfg)\n",
    "  # seed history with training dataset\n",
    "  history = [x for x in train]\n",
    "  # step over each time-step in the test set\n",
    "  for i in range(len(test)):\n",
    "  # fit model and make forecast for history\n",
    "    yhat = model_predict(model, history, cfg)\n",
    "    # store forecast in list of predictions\n",
    "    predictions.append(yhat)\n",
    "    # add actual observation to history for the next loop\n",
    "    history.append(test[i])\n",
    "  # estimate prediction error\n",
    "  error = measure_rmse(test, predictions)\n",
    "  print(' > %.3f' % error)\n",
    "  return error\n",
    "\n",
    "# repeat evaluation of a config\n",
    "def repeat_evaluate(data, config, n_test, n_repeats=30):\n",
    "  # fit and evaluate the model n times\n",
    "  scores = [walk_forward_validation(data, n_test, config) for _ in range(n_repeats)]\n",
    "  return scores\n",
    "\n",
    "# summarize model performance\n",
    "def summarize_scores(name, scores):\n",
    "  # print a summary\n",
    "  scores_m, score_std = mean(scores), std(scores)\n",
    "  print('%s: %.3f RMSE (+/- %.3f)' % (name, scores_m, score_std))\n",
    "  # box and whisker plot\n",
    "  pyplot.boxplot(scores)\n",
    "  pyplot.show()\n",
    "\n",
    "series = read_csv('B2BForecast2.csv', header=0, index_col=0)\n",
    "data = series.values\n",
    "# data split\n",
    "n_test = 12\n",
    "# define config\n",
    "config = [24, 500, 100, 100]\n",
    "# grid search\n",
    "scores = repeat_evaluate(data, config, n_test)\n",
    "# summarize scores\n",
    "summarize_scores('mlp', scores)\n",
    "\n",
    "series_to_supervised(data, n_in=3, n_out=1)\n",
    "train = data[:-n_test]\n",
    "test = data[-n_test:]\n",
    "model_fit(train,config)\n",
    "walk_forward_validation(data, n_test, config)\n",
    "history = [x for x in train]\n",
    "ff = model_predict(model_fit(train, config), history, config)\n",
    "print(ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_64 (Dense)             (None, 500)               12500     \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 1)                 501       \n",
      "=================================================================\n",
      "Total params: 13,001\n",
      "Trainable params: 13,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_64_input to have shape (24,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-444518184196>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#history = np.array(history).reshape(1, 24)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m#print (result[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    427\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     x, y, sample_weights = standardize(\n\u001b[0;32m--> 646\u001b[0;31m         x, y, sample_weight=sample_weights)\n\u001b[0m\u001b[1;32m    647\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2383\u001b[0;31m         batch_size=batch_size)\n\u001b[0m\u001b[1;32m   2384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2408\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2409\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2410\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2412\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    580\u001b[0m                              \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m                              str(data_shape))\n\u001b[0m\u001b[1;32m    583\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_64_input to have shape (24,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "model = tf.keras.models.load_model('g72dkV4aChicken-Data-2019.h5')\n",
    "# summarize model.\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.summary()\n",
    "v = np.array([200, 100, 100, 400, 200, 100, 100, 400,200, 100, 100, 400, 200, 100, 100, 400, 200, 100, 100, 400, 230, 444, 230, 675])\n",
    "x_input = array(v).reshape(1, 24)\n",
    "series = pd.read_csv('c.csv', header=0, index_col=0)\n",
    "data = series.values\n",
    "train = data[:-12]\n",
    "test = data[-12:]\n",
    "history = [x for x in train]\n",
    "#history = np.array(history).reshape(1, 24)\n",
    "result = model.predict(history, verbose=3)\n",
    "#print (result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
